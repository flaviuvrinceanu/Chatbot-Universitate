{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abefe643",
   "metadata": {},
   "source": [
    "# UTCN Chatbot \n",
    "\n",
    "- Upload `chatbot_dataset.jsonl` (keys: `instruction`, `input` (optional), `output`).\n",
    "- Accept LLaMA‑2 license on Hugging Face and use a token with access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1a17e0",
   "metadata": {},
   "source": [
    "## 1) Install pinned dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f4ac2-67de-4276-9f0f-a3d7e01440fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:39:55.827101Z",
     "iopub.status.busy": "2025-08-27T17:39:55.826825Z",
     "iopub.status.idle": "2025-08-27T17:42:44.508175Z",
     "shell.execute_reply": "2025-08-27T17:42:44.507004Z",
     "shell.execute_reply.started": "2025-08-27T17:39:55.827080Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping trl as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.4.1 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m Installed pinned deps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip uninstall -y torch torchvision torchaudio transformers bitsandbytes triton accelerate peft trl sentence-transformers\n",
    "\n",
    "\n",
    "!pip install --index-url https://download.pytorch.org/whl/cu121 \\\n",
    "  torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1\n",
    "\n",
    "\n",
    "!pip install \\\n",
    "  \"transformers==4.45.2\" \"trl==0.9.6\" \"peft==0.12.0\" \\\n",
    "  \"accelerate>=0.34.2\" \"bitsandbytes==0.45.2\" \\\n",
    "  \"safetensors>=0.4.3\" \"sentencepiece>=0.2.0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476ab418",
   "metadata": {},
   "source": [
    "## 3) Load local JSONL dataset and format prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d977d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:42:48.916859Z",
     "iopub.status.busy": "2025-08-27T17:42:48.915968Z",
     "iopub.status.idle": "2025-08-27T17:42:50.695464Z",
     "shell.execute_reply": "2025-08-27T17:42:50.694696Z",
     "shell.execute_reply.started": "2025-08-27T17:42:48.916819Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 303 examples\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 272\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 31\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import json, os, random\n",
    "from datasets import Dataset, DatasetDict\n",
    "random.seed(9)\n",
    "\n",
    "LOCAL_JSONL_PATH =  \"chatbot_dataset.jsonl\"\n",
    "assert os.path.exists(LOCAL_JSONL_PATH), f\"Upload your dataset to {LOCAL_JSONL_PATH}\"\n",
    "\n",
    "rows = []\n",
    "with open(LOCAL_JSONL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line=line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        ex = json.loads(line)\n",
    "        if \"instruction\" in ex and \"output\" in ex:\n",
    "            rows.append(ex)\n",
    "\n",
    "print(f\"Loaded {len(rows)} examples\")\n",
    "\n",
    "\n",
    "SYS_RO = \"Ești asistentul UTCN. Răspunde clar, corect și concis pentru studenți.\"\n",
    "SYS_EN = \"You are the UTCN assistant. Answer clearly, correctly, and concisely for students.\"\n",
    "\n",
    "def format_example(ex):\n",
    "    inst = (ex.get(\"instruction\") or \"\").strip()\n",
    "    inp  = (ex.get(\"input\") or \"\").strip()\n",
    "    out  = (ex.get(\"output\") or \"\").strip()\n",
    "    sys_prompt = SYS_RO if any(ch in inst for ch in \"ăâîșșțţĂÂÎȘȘȚŢ\") else SYS_EN\n",
    "    if inp:\n",
    "        prompt = f\"<<SYS>>{sys_prompt}<</SYS>>\\nUser: {inst}\\n{inp}\\nAssistant:\"\n",
    "    else:\n",
    "        prompt = f\"<<SYS>>{sys_prompt}<</SYS>>\\nUser: {inst}\\nAssistant:\"\n",
    "    return {\"text\": prompt + \" \" + out}\n",
    "\n",
    "formatted = [format_example(x) for x in rows]\n",
    "ds = Dataset.from_list(formatted).train_test_split(test_size=0.1, seed=9)\n",
    "ds = DatasetDict(train=ds[\"train\"], test=ds[\"test\"])\n",
    "print(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb30a4",
   "metadata": {},
   "source": [
    "## 4) Load Llama2 and attach LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10349c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:42:52.192289Z",
     "iopub.status.busy": "2025-08-27T17:42:52.192026Z",
     "iopub.status.idle": "2025-08-27T17:45:15.653658Z",
     "shell.execute_reply": "2025-08-27T17:45:15.653008Z",
     "shell.execute_reply.started": "2025-08-27T17:42:52.192269Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7ca32dfa9543378d6173ea7edce3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5ae36589d0405d953cfe29cbd7ad25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8053f2a5924c4d8310aba1c5bc412d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043c727b9ec9480480be08fec8985945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0108d2d0d1e4c7db91ef43f319fda28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f51f94c017f46e4b0e407f14a975fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f017d88be761406183d8c9bc864e9e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e9b9dc9e2d4ac9b393810b54ee750d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e07f3858f24399b4a4cf291f62e229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ef29d7f1404535a8274d801c659cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b72116b4e24054a0444913d18cd6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,777,216 || all params: 6,755,192,832 || trainable%: 0.2484\n",
      " Model ready\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "BASE = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "HF_TOKEN = \"-\"\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8 else torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE, use_fast=True,token=HF_TOKEN)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE,\n",
    "    token=HF_TOKEN,\n",
    "    quantization_config=bnb,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "print(\" Model ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efd10d1",
   "metadata": {},
   "source": [
    "## 5) Train with TRL SFTTrainer (response‑only loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef858cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:48:30.385755Z",
     "iopub.status.busy": "2025-08-27T17:48:30.385422Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8feaa836fa443d08e51133c6c76280a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/272 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b31a48cba44756a48d286003a07c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4/17 00:33 < 03:37, 0.06 it/s, Epoch 0.18/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "from transformers import TrainingArguments  \n",
    "import torch\n",
    "\n",
    "OUTPUT_DIR = \"utcn_lora_out\"\n",
    "\n",
    "\n",
    "response_template_ids = tokenizer.encode(\"\\nAssistant:\", add_special_tokens=False)\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)\n",
    "\n",
    "cfg = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=20,\n",
    "    bf16=(torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8),\n",
    "    fp16=not (torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8),\n",
    "    optim=\"paged_adamw_8bit\",     \n",
    "    gradient_checkpointing=True,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",       \n",
    "    report_to=\"none\",\n",
    "\n",
    "    \n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    packing=False,                \n",
    ")\n",
    "\n",
    "IGNORE = -100\n",
    "def to_ids_with_labels(example):\n",
    "    text = example[\"text\"].replace(\"\\r\\n\", \"\\n\")\n",
    "    i = text.find(\"Assistant:\")\n",
    "    assert i != -1, \"Separator missing\"\n",
    "    prompt = text[:i+len(\"Assistant:\")]\n",
    "    completion = text[i+len(\"Assistant:\"):]\n",
    "    ids_p = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n",
    "    ids_c = tokenizer(completion, add_special_tokens=False)[\"input_ids\"]\n",
    "    return {\"input_ids\": ids_p + ids_c,\n",
    "            \"labels\":    [IGNORE]*len(ids_p) + ids_c}\n",
    "    \n",
    "\n",
    "proc = ds.map(to_ids_with_labels, remove_columns=[\"text\"])\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.truncation_side = \"right\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,                \n",
    "    max_length=None,             \n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8        \n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=cfg,\n",
    "    train_dataset=proc[\"train\"], \n",
    "    eval_dataset=proc[\"test\"],\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\" Saved adapters to\", OUTPUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8153792,
     "sourceId": 12887589,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
