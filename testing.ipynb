{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaac94d0",
   "metadata": {},
   "source": [
    "# UTCN Llama-2 \n",
    "\n",
    "This notebook loads the **base model** and your **LoRA adapters**, then provides a simple `generate()` helper to test prompts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a0bba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.18\n",
      "PyTorch: 2.4.1+cu121\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39\n"
     ]
    }
   ],
   "source": [
    "import torch, platform, sys\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"Platform:\", platform.platform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5006a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "\n",
    "tok = \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d17e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2fbf567ab04d0baa6e9efcc4f53cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded base + adapters\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE = \"meta-llama/Llama-2-7b-chat-hf\"   \n",
    "ADAPTER_DIR = \"utcn_lora_out\"            \n",
    "\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8 else torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE, use_fast=True, token=tok)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.truncation_side = \"right\"\n",
    "tokenizer.model_max_length = 4096\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE,\n",
    "    token=tok,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb,\n",
    "    trust_remote_code=False\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base, ADAPTER_DIR, is_trainable=False)\n",
    "model.eval()\n",
    "print(\" Loaded base + adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94765bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers ready. Call generate('Your question', lang='en'|'ro')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Grounded prompt helpers\n",
    "\n",
    "\n",
    "OFFICIAL_URLS = {\n",
    "    \"sinu_portal\":    \"https://websinu.utcluj.ro/\",\n",
    "    \"main_site_en\":   \"https://www.utcluj.ro/en/\",\n",
    "    \"main_site_ro\":   \"https://www.utcluj.ro/\",\n",
    "    \"admission\":      \"https://admitereonline.utcluj.ro/\",\n",
    "    \"digital\":        \"https://utcluj.digital/\",\n",
    "}\n",
    "\n",
    "WHITELIST_DOMAINS = {\n",
    "    \"utcluj.ro\", \"websinu.utcluj.ro\",\n",
    "    \"admitereonline.utcluj.ro\", \"utcluj.digital\"\n",
    "}\n",
    "\n",
    "SYSTEM_EN = (\n",
    "    \"You are the UTCN assistant. Answer clearly, correctly, and concisely for students. \"\n",
    "    \"Use only the official domains: utcluj.ro, websinu.utcluj.ro, \"\n",
    "    \"admitereonline.utcluj.ro, utcluj.digital. \"\n",
    "    \"If you are not sure, say you’re not sure. Do not invent URLs or dates. Do not use HTML.\"\n",
    ")\n",
    "SYSTEM_RO = (\n",
    "    \"Ești asistentul UTCN. Răspunde clar, corect și concis pentru studenți. \"\n",
    "    \"Folosește doar domeniile oficiale: utcluj.ro, websinu.utcluj.ro, \"\n",
    "    \"admitereonline.utcluj.ro, utcluj.digital. \"\n",
    "    \"Dacă nu ești sigur, spune că nu ești sigur. Nu inventa URL-uri sau date. Nu folosi HTML.\"\n",
    ")\n",
    "\n",
    "def grounding_facts(lang: str = \"en\") -> str:\n",
    "    if lang.lower().startswith(\"ro\"):\n",
    "        return (\n",
    "            \"FAPTE:\\n\"\n",
    "            f\"- Portal studenti - SINU (autentificare): {OFFICIAL_URLS['sinu_portal']}\\n\"\n",
    "            f\"- Site principal: {OFFICIAL_URLS['main_site_ro']}\\n\"\n",
    "            f\"- Admitere: {OFFICIAL_URLS['admission']}\\n\"\n",
    "            f\"- UTCluj.Digital: {OFFICIAL_URLS['digital']}\\n\"\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            \"FACTS:\\n\"\n",
    "            f\"- SINU (login): {OFFICIAL_URLS['sinu_portal']}\\n\"\n",
    "            f\"- Main site (EN): {OFFICIAL_URLS['main_site_en']}\\n\"\n",
    "            f\"- Admissions: {OFFICIAL_URLS['admission']}\\n\"\n",
    "            f\"- UTCluj.Digital: {OFFICIAL_URLS['digital']}\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "def build_prompt(user_text: str, lang: str = \"en\") -> str:\n",
    "    sys_prompt = SYSTEM_RO if lang.lower().startswith(\"ro\") else SYSTEM_EN\n",
    "    facts = grounding_facts(lang)\n",
    "    return f\"<<SYS>>{sys_prompt}\\n{facts}<</SYS>>\\nUser: {user_text}\\nAssistant:\"\n",
    "\n",
    "\n",
    "STOP_STRINGS = [\"\\nUser:\", \"\\nQuestion:\", \"<<SYS>>\", \"[/INST]\"]\n",
    "\n",
    "class KeywordStopper(StoppingCriteria):\n",
    "    def __init__(self, keywords, tokenizer):\n",
    "        self.encoded = [tokenizer.encode(k, add_special_tokens=False) for k in keywords]\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        seq = input_ids[0].tolist()\n",
    "        for k in self.encoded:\n",
    "            if len(seq) >= len(k) and seq[-len(k):] == k:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def strip_html(text: str) -> str:\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    text = html.unescape(text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(user_text: str, lang: str = \"en\", max_new_tokens=200, temperature=0.7, top_p=0.9, do_sample=False):\n",
    "    prompt = build_prompt(user_text, lang=lang)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    stopper = StoppingCriteriaList([KeywordStopper(STOP_STRINGS, tokenizer)])\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=do_sample,\n",
    "        repetition_penalty=1.1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        stopping_criteria=stopper,\n",
    "    )\n",
    "    full = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    ans = full.split(\"Assistant:\", 1)[-1] if \"Assistant:\" in full else full\n",
    "    for s in STOP_STRINGS:\n",
    "        if s in ans:\n",
    "            ans = ans.split(s, 1)[0]\n",
    "    return strip_html(ans).strip()\n",
    "\n",
    "print(\"Helpers ready. Call generate('Your question', lang='en'|'ro')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f668a074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can log in to the student portal at https://websinu.utcluj.ro/ using your SINU credentials.\n",
      "---\n",
      "Autentifica pe https://websinu.utcluj.ro/\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"Where do I log in to the student portal?\", lang=\"en\"))\n",
    "print(\"---\")\n",
    "print(generate(\"Unde mă autentific în portalul studenților ?\", lang=\"ro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2656cd4c-9f1f-4449-86b1-2ad9af09e91e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
